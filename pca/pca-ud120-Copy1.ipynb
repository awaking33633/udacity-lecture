{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is used thoroughly for most of the time in visualization data, alongside feature set compression. It's hard (othwerwise impossible) to interpret the data with more than three dimension. So we reduce it to two/third dimension, allow us to make the visualization. This could come very handy if we want to capture the general information about the data, or to present it as business context. But it's not the tool that we use for combining two features, or compress it, as PCA throws some (important) information.\n",
    "<!-- TEASER_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first case the data is well distributed among 2d graph. For the two and third case, the data is actually drawn for 1 line graph, and we can say that it's actually still one dimension. The third graph, while it's true for 2d, but we can still capture the trends of the data by drawing straight line, and treat deviation as noise. The last graph is what we can consider as 2D graph. It may has definite curve, but we can only draw straight line to reduce the dimension using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the fundamentals of mathematical process behind PCA, what to do, and advice please visit my other [blog posts](http://napitupulu-jon.appspot.com/categories/dimensionality-reduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How PCA works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA works as these simple steps. Assign new x prime in horizontal line (trends of the data), and put the orthogonal as the ones that have less noise.\n",
    "\n",
    "These new data center have  it's own average weight. We can see the underlying center point has an x prime(delta x/y) and y prime(delta x/y). These will be the fundamentals of location in reduced dimension later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data in the graph is PCA-ready.The PCA also treats asymmetrical lines, which doesn't care input/output (third graph in this particular), vertical/horizontal, the PCA doesn't entail 1D graph to reduce the dimension. Well I say, that's pretty cool! Haha :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for which of major axis dominates, we know that from seeing the graph first and third, it's a clear winner, which eigenvalues are bigger from another. The first graph have x with bigger eigenvalue than y, the orthogonal, which has zero. And so for the third graph. Interesting enough, the graph in the middle have same eigenvalues, same magnitude which gives no clear major axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have 4 features (square ft, number of rooms, school ranking, and the safety problems) to predict the price of a house.  Considering maybe you want to have visualization of 2 dimension, you truncate all features to 2 dimension, size and neighborhood. If you don't know any number of features and only want to take n features, you want to choose SelectKBest from sklearn, or SelectPercentile to have n-percentage from features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that you want to create these composite feature, as your machine learning feature. This is what PCA means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the difference between PCA and regression (you may want to check this [post](http://napitupulu-jon.appspot.com/posts/Principal-Component-Analysis-problem-formulation.html). In PCA, you take the perpendicular of a point projected to the line. This is why PCA may not be used to hone the regression. It only used to make visualization and get better insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have \"safety problems\" and \"school ranking\" that we want to latent one feature in the data, maybe  \"neighborhood.The PCA must be choosing whichever direction of a projection line that retain most of the information as it can. Here PCA chose red one over the blue line, because the red one minimize sum of error over all data points. With this projection, we have retains much of information as we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the PCA works as unsupervised learning algorithm. For perhaps infinite number of features, it cluster the category of those features. Here's PCA cluster 4 features to 2 features, size and neighborhood. As it caught 2 trends among all these features. It makes as a regression to predict the housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Enron Finance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jpeg](../galleries/pca/14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, because this blog post are the note that I have taken from Udacity course, you can see the link of the course for this note at the bottom of the page. Here I attack some of the problem they have at their mini project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our discussion of PCA spent a lot of time on theoretical issues, so in this mini-project we’ll ask you to play around with some sklearn code. The eigenfaces code is interesting and rich enough to serve as the testbed for this entire mini-project.\n",
    "\n",
    "The starter code can be found in pca/eigenfaces.py. This was mostly taken from the example found here, on the sklearn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================\n",
      "Faces recognition example using eigenfaces and SVMs\n",
      "===================================================\n",
      "\n",
      "The dataset used in this example is a preprocessed excerpt of the\n",
      "\"Labeled Faces in the Wild\", aka LFW_:\n",
      "\n",
      "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
      "\n",
      "  .. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
      "\n",
      "  original source: http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html\n",
      "\n",
      "\n",
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n",
      "Extracting the top 150 eigenfaces from 966 faces\n",
      "done in 0.448s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.080s\n",
      "Fitting the classifier to the training set\n",
      "done in 30.120s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n",
      "  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,\n",
      "  random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
      "Predicting the people names on the testing set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ffb6f0e7467f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m \u001b[0mpcaTrainAndPredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;31m###############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;31m# Qualitative evaluation of the predictions using matplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-ffb6f0e7467f>\u001b[0m in \u001b[0;36mpcaTrainAndPredict\u001b[1;34m(n_components)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;31m#my add 3 lines into function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     prediction_titles = [title(y_pred, y_test, target_names, i)\n\u001b[1;32m--> 117\u001b[1;33m                          for i in range(y_pred.shape[0])]\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mplot_gallery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_titles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'title' is not defined"
     ]
    }
   ],
   "source": [
    "# %load eigenfaces.py\n",
    "\n",
    "\"\"\"\n",
    "===================================================\n",
    "Faces recognition example using eigenfaces and SVMs\n",
    "===================================================\n",
    "\n",
    "The dataset used in this example is a preprocessed excerpt of the\n",
    "\"Labeled Faces in the Wild\", aka LFW_:\n",
    "\n",
    "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
    "\n",
    "  .. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "  original source: http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print __doc__\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "np.random.seed(42)\n",
    "\n",
    "# fot machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print \"Total dataset size:\"\n",
    "print \"n_samples: %d\" % n_samples\n",
    "print \"n_features: %d\" % n_features\n",
    "print \"n_classes: %d\" % n_classes\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "def pcaTrainAndPredict(n_components):\n",
    "    ###############################################################################\n",
    "    # Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "    # dataset): unsupervised feature extraction / dimensionality reduction\n",
    "#     n_components = 150\n",
    "\n",
    "    print \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0])\n",
    "    t0 = time()\n",
    "    pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "\n",
    "    eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "    print \"Projecting the input data on the eigenfaces orthonormal basis\"\n",
    "    t0 = time()\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # Train a SVM classification model\n",
    "\n",
    "    print \"Fitting the classifier to the training set\"\n",
    "    t0 = time()\n",
    "    param_grid = {\n",
    "             'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "              }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)\n",
    "    clf = clf.fit(X_train_pca, y_train)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "    print \"Best estimator found by grid search:\"\n",
    "    print clf.best_estimator_\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "    print \"Predicting the people names on the testing set\"\n",
    "    t0 = time()\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    \n",
    "    #my add 3 lines into function\n",
    "    prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                         for i in range(y_pred.shape[0])]\n",
    "\n",
    "    plot_gallery(X_test, prediction_titles, h, w)\n",
    "    \n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "\n",
    "    print classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print confusion_matrix(y_test, y_pred, labels=range(n_classes))\n",
    "\n",
    "\n",
    "pcaTrainAndPredict(150)\n",
    "###############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        pl.subplot(n_row, n_col, i + 1)\n",
    "        pl.imshow(images[i].reshape((h, w)), cmap=pl.cm.gray)\n",
    "        pl.title(titles[i], size=12)\n",
    "        pl.xticks(())\n",
    "        pl.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "# prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "#                          for i in range(y_pred.shape[0])]\n",
    "\n",
    "# plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['title']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "===================================================\n",
      "Faces recognition example using eigenfaces and SVMs\n",
      "===================================================\n",
      "\n",
      "The dataset used in this example is a preprocessed excerpt of the\n",
      "\"Labeled Faces in the Wild\", aka LFW_:\n",
      "\n",
      "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
      "\n",
      "  .. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
      "\n",
      "  original source: http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html\n",
      "\n",
      "\n",
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n",
      "Extracting the top 150 eigenfaces from 966 faces\n",
      "done in 0.392s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.080s"
     ]
    }
   ],
   "source": [
    "# %%writefile eigenfaces.py\n",
    "\n",
    "\"\"\"\n",
    "===================================================\n",
    "Faces recognition example using eigenfaces and SVMs\n",
    "===================================================\n",
    "\n",
    "The dataset used in this example is a preprocessed excerpt of the\n",
    "\"Labeled Faces in the Wild\", aka LFW_:\n",
    "\n",
    "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
    "\n",
    "  .. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "  original source: http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print __doc__\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "np.random.seed(42)\n",
    "\n",
    "# fot machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print \"Total dataset size:\"\n",
    "print \"n_samples: %d\" % n_samples\n",
    "print \"n_features: %d\" % n_features\n",
    "print \"n_classes: %d\" % n_classes\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "def pcaTrainAndPredict(n_components):\n",
    "    ###############################################################################\n",
    "    # Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "    # dataset): unsupervised feature extraction / dimensionality reduction\n",
    "#     n_components = 150\n",
    "\n",
    "    print \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0])\n",
    "    t0 = time()\n",
    "    pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "\n",
    "    eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "    print \"Projecting the input data on the eigenfaces orthonormal basis\"\n",
    "    t0 = time()\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # Train a SVM classification model\n",
    "\n",
    "    print \"Fitting the classifier to the training set\"\n",
    "    t0 = time()\n",
    "    param_grid = {\n",
    "             'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "              }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)\n",
    "    clf = clf.fit(X_train_pca, y_train)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "    print \"Best estimator found by grid search:\"\n",
    "    print clf.best_estimator_\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "    print \"Predicting the people names on the testing set\"\n",
    "    t0 = time()\n",
    "    print \"X_test_pca:\",X_test_pca\n",
    "    print  \"clf.predict(X_test_pca):\", clf.predict(X_test_pca)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    print y_pred\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "\n",
    "    print classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print confusion_matrix(y_test, y_pred, labels=range(n_classes))\n",
    "\n",
    "\n",
    "pcaTrainAndPredict(150)\n",
    "###############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        pl.subplot(n_row, n_col, i + 1)\n",
    "        pl.imshow(images[i].reshape((h, w)), cmap=pl.cm.gray)\n",
    "        pl.title(titles[i], size=12)\n",
    "        pl.xticks(())\n",
    "        pl.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "# print \"y_pred:\",y_pred\n",
    "# prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "#                          for i in range(y_pred.shape[0])]\n",
    "\n",
    "# plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# # plot the gallery of the most significative eigenfaces\n",
    "\n",
    "# eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "# plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "# pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"y_pred:\",y_pred\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                         for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "# pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned that PCA will order the principal components, with the first PC giving the direction of maximal variance, second PC has second-largest variance, and so on. How much of the variance is explained by the first principal component? The second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll experiment with keeping different numbers of principal components. In a multiclass classification problem like this one (more than 2 labels to apply), accuracy is a less-intuitive metric than in the 2-class case. Instead, a popular metric is the F1 score.\n",
    "\n",
    "We’ll learn about the F1 score properly in the lesson on evaluation metrics, but you’ll figure out for yourself whether a good classifier is characterized by a high or low F1 score. You’ll do this by varying the number of principal components and watching how the F1 score changes in response.\n",
    "\n",
    "As you add more principal components as features for training your classifier, do you expect it to get better or worse performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change n_components to the following values: [10, 15, 25, 50, 100, 250]. For each number of principal components, note the F1 score for Ariel Sharon. (For 10 PCs, the plotting functions in the code will break, but you should be able to see the F1 scores.) If you see a higher F1 score, does it mean the classifier is doing better, or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any evidence of overfitting when using a large number of PCs? Does the dimensionality reduction of PCA seem to be helping your performance here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_n_components= [10, 15, 25, 50, 100, 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in list_n_components:\n",
    "    pcaTrainAndPredict(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score starts to drop, evidence of overfitting with n_components  = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **REFERENCE**:\n",
    "\n",
    "> * https://www.udacity.com/course/viewer#!/c-ud120/l-2962298545/e-3024958679/m-3015748699\n",
    "> * http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
